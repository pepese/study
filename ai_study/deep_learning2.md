深層学習 -Deep Learning-  
人工知能学会 監修 近代科学社

deep_learning.md に記載されていないことを中心にまとめる。

# 1. 階層型ニューラルネットワークに置ける深層学習

### 1.2.2. 特徴工学と表現学習

特徴ベクトルの評価の観点として以下がある。

- 情報量
  - 入力信号の情報量でできるだけ多く保存していること
- 独立性
  - 相互の特徴ができるだけ独立で、情報に重複がないこと
- 説明性
  - どのような情報が抽出されているかが解釈・説明しやすいこと
- スパース性
  - ゼロでない値を取る特徴の数が少ないこと
- 不変性
  - 入力信号の特定の変換に対して変化しないこと
- ロバスト性
  - 入力信号の微小変動に対して変化しないこと
- 平滑性
  - 元の情報が変化したときに特徴の値が滑らかに変化すること

上記には違いに相反するものも含まれている。例えば、説明性や独立性を重視すると、情報量は犠牲になる可能性がある。  
上記の特徴をコントロールする際に以下のような手法が使われる場合がある。

- 主成分分析（principal component analysis; PCA）
  - 次元削減後の分散が最大になる軸にデータを射影することで低次元化を行う
- 因子分解（factor analysis; FA）
  - 主成分分析の軸を回転させて、より解釈しやすい特徴を検索する
- 独立成分分析（independent component analysis; ICA）
  - 独立性の高い特徴を抽出する

## 1.7. 自己符号化器

自己符号化器には以下がある。

- 積層自己符号化器
  - 隠れ層が1層しかない自己符号化器を学習させ、出力層を排除し、隠れ層から出力される情報を次に新たに追加する隠れそうの入力となるようにして再び学習させ・・・というふうに、学習、隠れ層の追加、を繰り返す方法
- スパース自己符号化器
  - 隠れ層に正規化項を導入してスパース性をコントロールした自己符号化器
- 雑音除去自己符号化器
  - 入力信号い
