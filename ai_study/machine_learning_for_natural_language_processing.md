自然言語処理シリース1  
言語処理のための機械学習入門

# 1. 必要な数学的知識
ここでは以下を学ぶ。
- 最適化問題
- 確率論
- 情報理論

## 1.1. 準備と本書における約束事
言語処理におけるタスクに以下がある。
- 単語分割（word segmentation）
- 品詞タグ付け（part-of-speech tagging）
- 構文解析（syntactic parsing）
- 文書分類（text classification）
- その他、照応分析、質問応答、機械翻訳など様々

機械学習とはデータから分類規則などを学ぶ枠組みであるため、データは必須である。言語処理では多くの場合、言語のさまざまな形の用例の集まりである **コーパス** （ **corpus** ）と呼ばれる言語データを用いる。

## 1.2. 最適化問題
**最適化問題** （ **optimization problem** ）とは、ある制約のもとで関数を最適化（最大化あるいは最小化）する変数値とそのときの関数値を求める問題である。それぞれ **最大化問題** （ **maximization problem** ）、 **最小化問題** （ **minimization problem** ）という。  
最適化したい関数を最適化問題の **目的関数** （ **objective function** ）という。  
目的関数を最適化する手法には以下がある。
- **最急勾配法** （ **gradient method** ）
  - 最大化が目的であれば **最急上昇法** （ **gradient ascent method** ）、最小化が目的であれば **最急降下法** （ **gradient descent method** ）という
  - $x^{new} := x^{old}+\epsilon\bigtriangledown_xf(x^{old})$
    - 上記の式で $x$ を更新していく
    - $\epsilon$ は **学習率** （ **learning rate** ）と呼ばれる正の定数、最急降下法の場合は $+$ が $-$ になる
- **ニュートン法**
  - $f(x)$ を最大化したい場合に用いる更新式
  - $x^{new}=x^{old}+\epsilon H_{x^{old}}^{-1}\bigtriangledown_xf(x^{old})$
    - $H_x$ はヘッセ行列でその逆行列を使用している
- **ラグランジュの乗数法** （ **the method of Lagrange mutilliers** ） / **ラグランジュの未定乗数法**
- 変数 $\lambda$ を導入した、下記の **ラグランジュ関数** （ **Lagrangian** ）を利用する
  - **$L(x,\lambda)=f(x)+\lambda g(x)$**
    - 変数 $\lambda$ を **ラグランジュ乗数** （ **Lagrange multiplier** ）と呼ぶ
  - **等式制約付凸計画問題** を解く際に利用する
    - 例えば、 $f(x)$ の最大化問題を $g(x)=0$ の制約付きで解く
    - <u>ラグランジュ関数の $x$ に関する偏微分が $0$ であり、且つ、与えられた制約を満たす時、最適な解が得られる</u>
    - $\bigtriangledown_xf(x)+\lambda \bigtriangledown_x g(x)=0$ 且つ $g(x)=0$
    - ラグランジュの未定乗数法は以下で利用される
      - 確率分布のパラメータ推定、EMアルゴリズム、ナイーブベイズ分類器、サポートベクトルマシン、など
  - **不等式制約付凸計画問題** を解く際にも利用される
    - 例えば、 $f(x)$ の最大化問題を $g(x)\geqq0$ の制約付きで解く
    - <u>ラグランジュ関数 $L(x,\lambda)$ について、以下のような不等式を満たす点 $(x^*, \lambda^*)$ を **鞍点** （あんてん、 **saddle point** ）といい、凸計画問題において最適解である</u>
    - $L(x\leqq, \lambda^*)\leqq L(x^*, \lambda^*)\leqq L(x^*, \lambda)$
      - $L(x\leqq, \lambda^*)\leqq L(x^*, \lambda^*)$ について $x$ に関する最大化問題、 $L(x^*, \lambda^*)\leqq L(x^*, \lambda)$ について $\lambda$ に関する最小化問題として解く

## 1.3. 確率（省略）
## 1.4. 連続確率変数（省略）
## 1.5. パラメータ推定（省略）
## 1.6. 情報理論

情報を伝送するための理論的な基盤を記述した学問が情報理論である。この情報理論で扱われている基本的な概念が、言語処理にも登場する。

### 1.6.1. エントロピー
（一旦省略）

# 2. 文字および単語の数学的表現

文書をベクトルで表現する方法。

##2.1. タイプ、トークン

- **タイプ** / **単語タイプ**
  - 単語の種類のこと、重複は無い
- **トークン** / **単語トークン**
  - 単語のこと、同じ単語が複数出現した場合重複は許す

## 2.2. $n$ グラム

**$n$ グラム** （ **n-gram** ）とは、隣り合って出現した $n$ 個の文字・単語のことである。  
特に、 $n=1$ を **ユニグラム** （ **unigram** ）、 $n=2$ を **バイグラム** （ **bigram** ）、 $n=3$ を **トリグラム** （ **trigram** ）と呼ぶ。  
ユニグラムは <u>語順の情報が完全に失われる</u> が、バイグラム以降は残る。  
また、単語列の先頭と末尾に **ダミーの単語** （ **dummy word** ）をつけることによって、最初に出現する単語と最後に出現する単語の情報を捉えることがある。  
単語ではなく文字を単位とした $n$ グラムのことを **文字 $n$ グラム** 、文字ではなく形態素解析して分かち書きにした後の1つ1つの単語を単位とした $n$ グラムのことを **単語 $n$ グラム** （ **形態素 $n$ グラム** ）という。

## 2.3. 文書、文のベクトル表現

- **bags-of-words**
  - 文書に出現する単語を数え上げてベクトルにしたもの
- **二値ベクトル** （ **binary vector** ）
  - 文書に出現する単語数ではなく、出現するか否か（0 or 1）で表現したベクトル
- **bags-of-ngrams**
  - 単語ではなく、 $n$ グラム単位で数え上げた bags-of-words
  - 文字 $n$ グラムでも 単語 $n$ グラム（形態素 $n$ グラム）でもよい

## 2.4. 文書に対する前処理とデータスパースネス問題

前処理として以下のものがある。
- **ストップワード** （ **stopword** ）の除去
  - 話題の種類と関係を持たない単語、助詞などの除去
- **ステミング** （ **stemming** ）
  - 派生語などを同一の単語ととらえる作業
- **見出し語化** （ **lemmatization** ）
  - 動詞などの活用形のある品詞を基本形に戻す作業

文書のベクトル化を例えば 50000 語ある辞書をもとに単語 $n$ グラムの bags-of-unigram で表現した場合、 100 語からなる文書をベクトル化したら少なくとも 49900 個の要素は 0 になる。
一般に、 0 でない値をとる要素数が小さい傾向にあるとき、このデータは **疎** （ **sparse** ）であるという。  
データが疎である場合、そのデータを処理するために必要な統計値が十分に角取るできないことがあり、この問題を一般的に **データスパースネス問題** （ **data sparseness problem** ）と呼ぶ。

## 2.5. 単語のベクトル表現
### 2.5.1. 単語トークンの文脈ベクトル表現
ある文書でベクトル化したい単語トークン以外の単語に着目して作成するベクトルを **文脈ベクトル** （ **context vector** ）という。  
例えば以下の例文を考える。
- 高く _ **跳ぶ** _ に _ は _ まず _ 屈め

「跳ぶ」という単語トークンは以下のように表すことができる。  
$n(w)$ は「跳ぶ」の前後 1 つの単語トークンに $w$ が存在するか否かを表す。

$$x_{跳ぶ}=(n(高く),n(に),n(は),n(まず),n(屈め))\\=(1,1,0,0,0)$$

対象単語の前後の数トークンを考慮しているとき、考慮している箇所を **文脈窓** （ **context window** ）、またその大きさを **文脈窓幅** （ **context window size** ）という。  
文脈ベクトルはコーパスを用いて作成することになる。

### 2.5.2. 単語タイプの文脈ベクトル表現

単語トークンの文脈ベクトル表現のほぼ同じであるが、 1 点ことなる。それは文書の中に同じ単語が出現したときにセットで考慮すること。  
以下に例をあげる。

- Nothing ventured, nothing gained.

nothing についての文脈ベクトルを作成する。 nothing が 2 箇所にあることに注意する。 $n(w)$ は先ほどと同様、前後 1 単語トークンを文脈窓とする。（ $+1$ は nothing の後ろ、 $-1$ は nothing の前を意味する）

$$x_{nothing}=(n(ventured_{+1}),n(ventured_{-1}),n(gained_{+1}),n(gained_{-1}),n(","_{+1}),n(","_{-1}))\\=(1,0,1,0,0,1)$$

### 2.5.3. 文脈や単語の確率分布による表現（省略）

# 3. クラスタリング
## 3.1. 準備（省略）
## 3.2. 凝集型クラスタリング
階層的クラスタリングのことを **凝集型クラスタリング** （ **agglomerative clustering** ）や **ボトムアップクラスタリング** （ **bottom-up clustering** ）とも呼ぶ。  
クラスタ同士の類似度を測る方法には以下がある。
- **単連結法** （ **single-link method** ）
  - 2 つのクラスタで最も近い要素同士の距離を取る方法
- **完全連結法** （ **complete-link method** ）
  - 2 つのクラスタで最も遠い要素同士の距離を取る方法
- **重心法** （ **centroid method** ）
  - 2 つのクラスタで重心同士の距離を取る方法
## 3.3. k-平均法（省略）
## 3.4. 混合正規分布によるクラスタリング（省略）
## 3.5. EMアルゴリズム（ほぼ省略）
**Expectation-Mazimization アルゴリズム** （ **EMアルゴリズム** ）。  
**確率的潜在意味解析** （ **probabilistic latent seantic analysis** ; **PLSA** ）。  
**確率的潜在意味索引付け** （ **probabilistic latent semantic indexing** ; **PLSI** ）
## 3.6. クラスタリングにおける問題点や注意点
- クラスタリングはあまりうまくいかない
- ほぼベクトルの作り方次第
  - 意味で分けたいなら意味ベクトル、など
- **制限付きクラスタリング** や **半教師付き学習** などの発展手法もよい
- クラスタリングでは常に <u>クラスタ数</u> の問題がつきまとう
- 初期値により結果が変わる
- 計算量が多い

# 4. 分類
## 4.1. 準備（省略）
## 4.2. ナイーブベイズ分類器
**ナイープベイズ分類器** （ **naive bayes classifier** ）は確率に基づいた分類器であり、観測データ $d$ にたいして、 $P(c|d)$ が最大となるクラス $c\in C$ を出力する。ベイズの定理を用いる。

$$P(c|d)=\frac{P(c)P(d|c)}{P(d)}$$

上記で右辺が最大になるクラス $c$ を求める場合、分母 $P(d)$ はクラスに依存しないため、分子の **$P(c)P(d|c)$** の最大化問題を解くことになる。  
しかし、 $P(d|c)$ を計算することは簡単では無い。  
$d$ は文書であるので、単語の種類数とその組合せを考えると、起こりうる $d$ は膨大であり、あらゆる $d$ についてそれぞれがデータ中で何回起こるかを調べ、 $P(d|c)$ を最尤推定で求めるのは非現実的である。  
ナイーブベイズ分類器では、 <u>文書 $d$ に簡単化したモデルを仮定</u> して $P(d|c)$ の値を求める。  
モデルとしては以下の 2 種類ある。
- **多変数ベルヌーイモデル** （ **multivariate Bernoulli model** ）
- **多項モデル** （ **multinomial model** ）

## 4.3. サポートベクトルマシン
（結構省略）
### 4.3.5. 多値分類器への拡張
SVM は二値分類器である。これを多値分類器へ拡張する方法には以下がある。
- **one-versus-rest 法**
- **ペアワイズ法** （ **pairwise method** ）
## 4.4. カーネル法
SVM では、学習においても分類においても観測データの内積だけ計算できればよい。  
このように、内積だけを用いて学習や分類を行うことを **カーネル法** （ **kernel method** ）という。  
また、観測データの対を引数として、高次元の内積を与える関数を **カーネル関数** （ **kernel function** ）とよび、一般的に **$K$** で表す。  
カーネル関数には以下のようなものがある。
- **木構造カーネル** （ **tree kernel** ）
- **文字カーネル** （ **string kernel** ）
- **多項式カーネル** （ **polynomial kernel** ）
- **動径基底関数カーネル** （ **radial basis function kernel** ; **RBFカーネル** ）

## 4.5. 対数線形モデル（ほぼ省略）
**対数線形モデル** （ **log-liner model** ）は、SVM と並んでよく使われる。  
確率的な分類器であり、観測データがあるラベルを持つ確率を教えてくれる。  
言語処理においては、 **最大エントロピーモデル** （ **maximum entropy model** ）とよばえることも多い。

## 4.6. 素性選択（省略）
# 5. 系列ラベリング（省略）
# 6. 実験の仕方など（省略）
