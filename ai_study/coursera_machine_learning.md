# 機械学習の分類

## アプローチによる分類

- 機械学習
  - **識別モデル**
    - 確率分布の仮定なし
      - カーネル、最適化
    - データの生成過程を考慮せず、データで問題を直接解く
  - 代表例
    - SVM（カーネルトリック）
- 統計的気機械学習
  - **生成モデル**
    - 確率分布の仮定あり
      - （ベイズ）統計
    - クラス事後確率により識別

## 学習方法による分類

- 教師あり学習
  - 訓練セット（Training Data）を使ってモデルに学習を行う
- 教師なし学習
- 半教師あり学習
- 強化学習
- アンサンブル学習
  - みんなで教えあう

## 手法や課題設定による分類

- 回帰（Regression）
- 分類（Classification）
- クラスタリング（Clustering）
- レコメンデーション
- 異常検出
- 次元削減
- トピックモデル

## 言い方いろいろ

- 学習データ、訓練データ、教師データ
- テストデータ、評価データ
- バリデート、検証

# 回帰

## 線形回帰（Liner Regression Model）

線形回帰モデルは以下。

$$y = h_\theta(x)$$

$x$ を **説明変数** （独立変数、回帰変数）、 $y$ を **目的変数** （従属変数、応答変数）という。  
$h$ は学習後の関数でここでは **仮説** （Hypothesis） と呼ぶ。  
$\theta$ は学習によって値を調整する重み。  
また、訓練セット（Training Data）の $i$ 番目の要素を $(x^{(i)}, y^{(i)})$ 、 訓練セットの数を $m$ とする。  
**単回帰** （ $h_{\theta} = \theta_0 + \theta_1x$ ）の場合の **コスト関数** （Cost Function）を以下に定義する。

$$J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})$$

この目的関数が最小となる時の $h_\theta$ が最適な線形回帰モデルとなる。

### 最急降下法（Gradient Descent Algorithm）

学習により最適なパラメータ $\theta$ を求める手法の1つ。


# 分類

分類には以下の種類がある。

- バイナリ分類問題、2クラス分類問題
  - 対象を2つのクラスに分類する
  - $0$ と $1$ のように分類する
- マルチクラス分類問題
  - 対象を3つ以上の複数クラスに分類する
  - $0, 1, 2, 3,...$ のように分類する

後述するが、通常、バイナリ分類問題を複数回適用することでマルチクラス分類問題にも対応です。

## ロジスティック回帰

バイナリ分類問題を解く手法の1つ。  
ロジスティック回帰では仮説に **シグモイド関数** を用いる。

$$g(z) = \frac{1}{1+e^{-z}}$$

上記のシグモイド関数には以下の特徴がある。

- $z=0$ のとき $g(0) = \frac{1}{2} = 0.5$
- $z>0$ の領域で $z$ が大きくなるほど $1$ に漸近する
- $z<0$ の領域で $z$ が小さくなるほど $0$ に漸近する

<img src="http://mathtrain.jp/wp-content/uploads/2014/12/sigmoid-300x250.png" />

シグモイド関数を適用して仮説は以下のようになる。

$$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$$

この仮説を用いてどのようにバイナリ分類問題を解くのか。考え方を列挙する。

- 前提
  - $h_\theta(x) \geqq 0.5$ の場合 $y=1$ と考える
  - $h_\theta(x) < 0.5$ の場合 $y=0$ と考える
- $h_\theta(x) \geqq 0.5$ の時、 $\theta^Tx \geqq 0$ となるため上記は
  - $\theta^Tx \geqq 0$ の場合 $y=1$
  - $\theta^Tx < 0$ の場合 $y=0$

つまり、 **$\theta^Tx = 0$ が2クラスの境界面** となる。

<img src="https://cacoo.com/diagrams/SSNwCQfZus6tC3LU-04925.png" />

上記は、「 $\theta^Tx \geqq 0$ のときは×」「 $\theta^Tx < 0$ のときは○」と見ることができる。

> ### 活性化関数（activate function）
> シグモイド関数は、ニューラルネットワークにおける **活性化関数**（ **activate function** ）の一種。伝達関数（transfer function）とも呼ばれる。

### ロジスティック回帰のコスト関数

ロジスティック回帰のコスト関数には以下の特徴が必要になる。

- $y=1$ のとき、 $h_\theta(x)$ が $1$ に近くほどコストが下がる
- $y=0$ のとき、 $h_\theta(x)$ が $0$ に近くほどコストが下がる

この特徴を捉えたコストを対数を用いて表すと以下のようになる。

- $y=1$ のとき、 $Cost(h_\theta(x), y) = -\log (h_\theta(x))$
- $y=0$ のとき、 $Cost(h_\theta(x), y) = -\log (1-h_\theta(x))$

上記を1つの式にすると以下のようになる。

$$Cost(h_\theta(x), y) = -y\log (h_\theta(x))-(1-y)\log (1-h_\theta(x))$$

以上のことからコスト関数は以下のようになる。

$$J(\theta) = \frac{1}{m}\sum_{i=1}^m Cost(h_\theta(x^{(i)}), y^{(i)}) \\= -\frac{1}{m}\sum_{i=1}^m [y^{(i)}\log (h_\theta(x^{(i)}))+(1-y^{(i)})\log (1-h_\theta(x^{(i)}))]$$

上記を最急降下法などを用いて解く。

## ニューラルネットワーク
