Atomのパッケージ「markdown-preview-plus」「mathjax-wrapper」をインストールし、「markdown-preview」を検索して無効にすると数式が見れるようになる。  
「Ctrl + Shift + M」で「markdown-preview-plus」のプレビュー、さらに「Ctrl + Shift + X」で「mathjax-wrapper」のプレビューで数式が見れるようになる。  
[参考](http://qiita.com/noppefoxwolf/items/335323b98f0400a6f07d)

はじめてのパターン認識

# 第 1 章：はじめに

- 「何か」を判断することを **識別**
- 「有効な特徴」を抽出することを **特徴抽出**
- 「有効な特徴」と「何か」を結びつける規則を **識別規則**
- 識別規則を **学習** する

## 1.1 パターン認識とは

- 対象を観測し、 **識別** の手がかりとなる特徴量を測定することを、パターン認識の分野では **特徴抽出** という
- 抽出された特徴を並べてベクトルの形にまとめたものを **特徴ベクトル** とよぶ
- 特徴ベクトルを用いて **クラス** に分類する
- この分類するための規則を **識別規則**

識別に有効な特徴をいかに早く抽出するかがパターン認識成功の鍵。

- 識別規則は、入力データが所属する正しいクラスを同定するための規則
- 入力データとそのクラスの対応を関係を **学習データ** という

識別規則の学習がパターン認識成功のもう一つの鍵。

## 1.2 特徴の型

- **定性的特徴** ・・・ **非数値** データ
  - **名義尺度** （離散変数、カテゴリ変数） ・・・ クラスラベル（クラスにつける名前）
  - **順序尺度** ・・・ 「大中小」「高低」など
  - **比例尺度** ・・・ 原点が0の順序尺度
  - **間隔尺度** ・・・ 間隔が **等間隔**　の尺度、日付など
- **定量的特徴** ・・・ **数値** データ

定性的特徴を計算機上で表現する場合は **符号** を用いる。  
例えば、「0 と 1」「-1 と +1」など。  
クラス数が2つより多い場合は、0と1の2値変数をを用意し、クラスに対応する変数のみを1としてその他を0とする符号化を行う **ダミー変数** を用いる。  
「大中小」表すクラスを $β_1$ 、 $β_2$ 、 $β_3$ とすると、ベクトル $β$ は以下のようになる。

$β = (β_1, β_2, β_3)^T$

|| $β_1$ | $β_2$ | $β_3$ |
|:---:|:---:|:---:|:---:|
|大|1|0|0|
|中|0|1|0|
|小|0|0|1|

## 1.3 特徴ベクトル空間の次元の呪い

未知の複雑な関数を学習するために必要なデータが次元の増加とともに指数関数的に増加することを **次元の呪い** と呼ぶ。

# 第 2 章：識別規則と学習法の概要

学習データに含まれていなかったものも正しく識別する能力を **汎化能力** とよぶ。

## 2.1 識別規則と学習の分類

### 2.1.1 識別規則の構成法

識別規則は、入力データ $x$ からクラス $C_i \in Ω = \{C_1,...,C_K\}$ への写像であると言える。  
写像の実現方法には以下がある。

- 事後確率
  - パターン空間に確率分布を家庭し、事後確率が最大となるクラスへ分類する方法
  - **ベイズの最大事後確率法** が代表例
  - 学習データから確率分布のパラメータを推定する必要がある
- 距離
  - 入力ベクトル $x$ と各クラスの代表ベクトルとの距離を計算し、一番近いクラスへ分類する方法
  - **最近傍法** が代表例
- 関数
  - 関数 $f(x)$ の正負、最大値でクラスを決定する方法
  - **パーセプトロン型学習回路** や **サポートベクトルマシン** が代表例
  - 識別の為に用いる関数 $f(x)$ を **識別関数** と呼ぶ
- 決定木
  - 識別規則の真偽を順次適用することでクラスを決定する方法

### 2.1.2 教師付き学習

識別規則は入力データからクラスへの写像 $y=f(x)$ で表す。これは、入力ベクトル $x$ とパラメータ $w$ の **線形関数（内積）** で表現できる。

$$y = f(x;w) = w_1x_1 + ... + w_dx_d = w^Tx$$

学習の目的は、入力データ $x$ に対して写像 $y$ が正しいクラスに対応する出力となるようにパラメータ $w$ を調整すること。  
入力データに対して出力として正しいクラスに対応させた対を **教師データ** といい、この集合を **$t$** と表現する。（tearchの **t** ）  
2クラスの場合は **$t \in \{-1, +1\}$** 、3クラス以上の場合はダミー変数を用いて **$t = \{0,1,0,0,0,0,0\}$** のように表現する。これを **K対1の符号化** （ **1-of-K coding** ）という。（Kはクラス数）  
教師データの対の集合を **学習データセット** **$D_L$** と表現する。（Dataの **D** 、Learnの **L** ）  
学習後、識別関数は学習に使用しなかった **テストデータセット** **$D_T$** を用いて性能評価を行う。（Dataの **D** 、 Testの **T** ）

### 2.1.3　教師付き学習と線形回帰

識別関数を線形関数を用いてモデル化することを **線形回帰** という。  
$x$ を **説明変数** 、 $y$ を **被説明変数** という。

### 2.1.4　教師なし学習

入力データ間の距離や類似度、統計的な性質に基づいてクラスを自動的に生成（ **クラスタリング** ）する方法を **教師なし学習** あるいは **自己組織型学習** という。

## 2.2 汎化能力

- **汎化能力**
  - 未知のデータに対する識別能力
- **汎化誤差**
  - 未知のデータに対する識別の誤差

### 2.2.1 学習データとテストデータの作り方

- 全てのデータの集合を  **母集団** と呼び、母集団を分割して学習データセット $D_L$ とテストデータセット $D_T$ を作成する。
- $D_L$ $D_T$ それぞれのデータセットの特徴ベクトルの分布を **$p_L$** 、 **$p_T$** とする。
- $D_L$ を用いて設計し、$D_T$を用いてテストした際の誤り率を **$ε(p_L,p_T)$** と表す。
- 母集団を用いたd次元特徴の分布を **真の分布** とよび、 **$p$** で表すことにする。
- $p_L$ や $p_T$ は真の分布からランダムにサンプルされたもので、各特徴の平均や分散が $p$ とおなじになるとは限らない。このズレを **偏り** （ **バイアス** ）という。
- **真の誤り率** $ε(p,p)$ は真の分布 $p$ を用いて設計し、 $p$ を用いてテストした時の誤り率を表す。
- 母集団からサンプリングした同じデータを学習とテストに用いた際の誤り率を $ε(p_L,p_L)$ とし、 **再代入誤り率** と呼ぶ。

母集団を分割して学習とテストを行う手法の代表的なものとして以下のものがある。

#### (1)ホールドアウト法

母集団を分割して一方を学習に使い、他方をテストのために置いておき、誤り率の推定に使用する。これによって求められる誤り率を **ホールドアウト誤り率** という。学習精度と性能評価がトレードオフの関係にあるため、手元に大量のデータがない場合は良い性能評価を行うことができない。

```
|ooooooooooooo|xxxxxxxxxxxxx|
|Learning Data|  Test Data  |
```

#### (2)交差確認方 / 交差検定のことかな？

母集団を $m$ 個のグループに分割し、 $m−1$ 個のグループを用いて学習を行う。そのあと残りの1つのグループでテストを行う。これを全グループがテストに用いられるよう $m$ 回繰り返し、その誤り率の平均を性能予測値とする。 $i$ 番目のグループの誤り率を $ε_i$ とすると、誤識別率の予測値は $ε=∑^m_{i=1}ε_i$ と予測される。分割によってグループごとのデータのバイアスがかかる可能性があるので、分割方法を変えて交差確認法を繰り返す必要がある。

```
test1: |xx|oo|oo|oo|
test2: |oo|xx|oo|oo|
test3: |oo|oo|xx|oo|
test4: |oo|oo|oo|xx|

o : Learning Data
x : Test Data
```

#### (3)ジャックナイフ法

交差確認法においてデータ数とグループ数を等しくして行う。組み合わせが1つなので1回だけやればよい。

#### (4)ブートストラップ法

再代入誤り率バイアスを補正するために使用される。総数N個の母集団に対して、N個のデータを用いて設計し、N個のデータでテストすることで再代入誤り率を求め、真の誤り率との差分をとることでバイアスを明らかにする。最低でも50回程度のリサンプリングを繰り返して施行を重ねてバイアスの精度を高めていく。

### 2.2.2　汎化能力の評価法とモデル選択

学習データによってパラメータ調整を行い、誤り率を評価しても、目標以上の精度が出ない場合は識別関数を変える必要がある。  
誤り率が最も小さくなるパラメータを選択する方法を **モデル選択** という。

# 第 3 章：ベイズの識別規則

# 第 4 章：確率モデルと識別関数

# 第 5 章：k近傍法（kNN法）

# 第 6 章：線形識別関数

# 第 7 章：パーセプトロン型学習規則

# 第 8 章：サポートベクトルマシン

# 第 9 章：部分空間法

# 第 10 章：クラスタリング

# 第 11 章：識別器の組み合わせによる性能強化

# 付録：ベクトルと行列による微分
