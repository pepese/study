Atomのパッケージ「markdown-preview-plus」「mathjax-wrapper」をインストールし、「markdown-preview」を検索して無効にすると数式が見れるようになる。  
「Ctrl + Shift + M」で「markdown-preview-plus」のプレビュー、さらに「Ctrl + Shift + X」で「mathjax-wrapper」のプレビューで数式が見れるようになる。  
[参考](http://qiita.com/noppefoxwolf/items/335323b98f0400a6f07d)

はじめてのパターン認識

# 第 1 章：はじめに

- 「何か」を判断することを **識別**
- 「有効な特徴」を抽出することを **特徴抽出**
- 「有効な特徴」と「何か」を結びつける規則を **識別規則**
- 識別規則を **学習** する

## 1.1 パターン認識とは

- 対象を観測し、 **識別** の手がかりとなる特徴量を測定することを、パターン認識の分野では **特徴抽出** という
- 抽出された特徴を並べてベクトルの形にまとめたものを **特徴ベクトル** とよぶ
- 特徴ベクトルを用いて **クラス** に分類する
- この分類するための規則を **識別規則**

識別に有効な特徴をいかに早く抽出するかがパターン認識成功の鍵。

- 識別規則は、入力データが所属する正しいクラスを同定するための規則
- 入力データとそのクラスの対応を関係を **学習データ** という

識別規則の学習がパターン認識成功のもう一つの鍵。

## 1.2 特徴の型

- **定性的特徴** ・・・ **非数値** データ
  - **名義尺度** （離散変数、カテゴリ変数） ・・・ クラスラベル（クラスにつける名前）
  - **順序尺度** ・・・ 「大中小」「高低」など
  - **比例尺度** ・・・ 原点が0の順序尺度
  - **間隔尺度** ・・・ 間隔が **等間隔**　の尺度、日付など
- **定量的特徴** ・・・ **数値** データ

定性的特徴を計算機上で表現する場合は **符号** を用いる。  
例えば、「0 と 1」「-1 と +1」など。  
クラス数が2つより多い場合は、0と1の2値変数をを用意し、クラスに対応する変数のみを1としてその他を0とする符号化を行う **ダミー変数** を用いる。  
「大中小」表すクラスを $β_1$ 、 $β_2$ 、 $β_3$ とすると、ベクトル $β$ は以下のようになる。

$β = (β_1, β_2, β_3)^T$

|| $β_1$ | $β_2$ | $β_3$ |
|:---:|:---:|:---:|:---:|
|大|1|0|0|
|中|0|1|0|
|小|0|0|1|

## 1.3 特徴ベクトル空間の次元の呪い

未知の複雑な関数を学習するために必要なデータが次元の増加とともに指数関数的に増加することを **次元の呪い** と呼ぶ。

# 第 2 章：識別規則と学習法の概要

学習データに含まれていなかったものも正しく識別する能力を **汎化能力** とよぶ。

## 2.1 識別規則と学習の分類

### 2.1.1 識別規則の構成法

識別規則は、入力データ $x$ からクラス $C_i \in \Omega = \{C_1,...,C_K\}$ への写像であると言える。  
写像の実現方法には以下がある。

- 事後確率
  - パターン空間に確率分布を家庭し、事後確率が最大となるクラスへ分類する方法
  - **ベイズの最大事後確率法** が代表例
  - 学習データから確率分布のパラメータを推定する必要がある
- 距離
  - 入力ベクトル $x$ と各クラスの代表ベクトルとの距離を計算し、一番近いクラスへ分類する方法
  - **最近傍法** が代表例
- 関数
  - 関数 $f(x)$ の正負、最大値でクラスを決定する方法
  - **パーセプトロン型学習回路** や **サポートベクトルマシン** が代表例
  - 識別の為に用いる関数 $f(x)$ を **識別関数** と呼ぶ
- 決定木
  - 識別規則の真偽を順次適用することでクラスを決定する方法

### 2.1.2 教師付き学習

識別規則は入力データからクラスへの写像 $y=f(x)$ で表す。これは、入力ベクトル $x$ とパラメータ $w$ の **線形関数（内積）** で表現できる。

$$y = f(x;w) = w_1x_1 + ... + w_dx_d = w^Tx$$

学習の目的は、入力データ $x$ に対して写像 $y$ が正しいクラスに対応する出力となるようにパラメータ $w$ を調整すること。  
入力データに対して出力として正しいクラスに対応させた対を **教師データ** といい、この集合を **$t$** と表現する。（tearchの **t** ）  
2クラスの場合は **$t \in \{-1, +1\}$** 、3クラス以上の場合はダミー変数を用いて **$t = \{0,1,0,0,0,0,0\}$** のように表現する。これを **K対1の符号化** （ **1-of-K coding** ）という。（Kはクラス数）  
教師データの対の集合を **学習データセット** **$D_L$** と表現する。（Dataの **D** 、Learnの **L** ）  
学習後、識別関数は学習に使用しなかった **テストデータセット** **$D_T$** を用いて性能評価を行う。（Dataの **D** 、 Testの **T** ）

### 2.1.3　教師付き学習と線形回帰

識別関数を線形関数を用いてモデル化することを **線形回帰** という。  
$x$ を **説明変数** 、 $y$ を **被説明変数** という。

### 2.1.4　教師なし学習

入力データ間の距離や類似度、統計的な性質に基づいてクラスを自動的に生成（ **クラスタリング** ）する方法を **教師なし学習** あるいは **自己組織型学習** という。

## 2.2 汎化能力

- **汎化能力**
  - 未知データに対する識別能力
- **汎化誤差**
  - 未知データを識別した時の誤差

### 2.2.1 学習データとテストデータの作り方

- 全てのデータの集合を  **母集団** と呼び、母集団を分割して学習データセット $D_L$ とテストデータセット $D_T$ を作成する。
- $D_L$ $D_T$ それぞれのデータセットの特徴ベクトルの分布を **$p_L$** 、 **$p_T$** とする。
- $D_L$ を用いて設計し、$D_T$を用いてテストした際の誤り率を **$ε(p_L,p_T)$** と表す。
- 母集団を用いたd次元特徴の分布を **真の分布** とよび、 **$p$** で表すことにする。
- $p_L$ や $p_T$ は真の分布からランダムにサンプルされたもので、各特徴の平均や分散が $p$ とおなじになるとは限らない。このズレを **偏り** （ **バイアス** ）という。
- **真の誤り率** $ε(p,p)$ は真の分布 $p$ を用いて設計し、 $p$ を用いてテストした時の誤り率を表す。
- 母集団からサンプリングした同じデータを学習とテストに用いた際の誤り率を $ε(p_L,p_L)$ とし、 **再代入誤り率** と呼ぶ。

母集団を分割して学習とテストを行う手法の代表的なものとして以下のものがある。

#### (1)ホールドアウト法 / holdout法

母集団を分割して一方を学習に使い、他方をテストのために置いておき、誤り率の推定に使用する。  
これによって求められる誤り率を **ホールドアウト誤り率** という。  
学習精度と性能評価がトレードオフの関係にあるため、手元に大量のデータがない場合は良い性能評価を行うことができない。

```
|ooooooooooooo|xxxxxxxxxxxxx|
|Learning Data|  Test Data  |
```

#### (2)交差確認法 / 交差検証 / cross validation法

母集団を $m$ 個のグループに分割し、 $m−1$ 個のグループを用いて学習を行う。  
そのあと残りの1つのグループでテストを行う。  
これを全グループがテストに用いられるよう $m$ 回繰り返し、その誤り率の平均を性能予測値とする。  
$i$ 番目のグループの誤り率を $ε_{-i}$ とすると、誤識別率の予測値は $ε=∑^m_{i=1}ε_{-i}$ と予測される。  
分割によってグループごとのデータのバイアスがかかる可能性があるので、分割方法を変えて交差確認法を繰り返す必要がある。

```
test1: |xx|oo|oo|oo|
test2: |oo|xx|oo|oo|
test3: |oo|oo|xx|oo|
test4: |oo|oo|oo|xx|

o : Learning Data
x : Test Data
```

#### (3)一つ抜き法 / leave-one-out法 / ジャックナイフ法

交差確認法においてデータ数とグループ数を等しくして行う。  
組み合わせが1つなので1回だけやればよい。

#### (4)ブートストラップ法 / bootstrap法

再代入誤り率バイアスを補正するために使用される。  
総数N個の母集団に対して、N個のデータを用いて設計し、N個のデータでテストすることで再代入誤り率を求め、真の誤り率との差分をとることでバイアスを明らかにする。  
最低でも50回程度のリサンプリングを繰り返して施行を重ねてバイアスの精度を高めていく。

### 2.2.2　汎化能力の評価法とモデル選択

学習データによってパラメータ調整を行い、誤り率を評価しても、目標以上の精度が出ない場合は識別関数を変える必要がある。  
誤り率が最も小さくなるパラメータを選択する方法を **モデル選択** という。

# 第 3 章：ベイズの識別規則

## 3.1 ベイズの識別規則

### 3.1.1 最大事後確率基準

**ベイズの定理**

$$P(C_i|x) = \frac{p(x|C_i)}{p(x)} \times P(C_i)$$

|式|意味|
|:---|:---|
| $x$ | 観測データ。 |
| $C_i$ | $i$ 番目のクラス |
| $P$ | 大文字のピー。離散確率分布。 |
| $p$ | 小文字のピー。連続確率分布。 |
| $P(C_i|x)$ | $i$ 番目のクラスの **事後確率** 。観測データ $x$ が与えられた下で、それがクラス $C_i$ に属する条件付き確率。 |
| $p(x|C_i)$ | $i$ 番目のクラスの **クラス条件付き確率** （ **尤度** ）。母数を $C_i$ として、その中で観測データ $x$ の確率。 |
| $p(x)$ | $p(x) = \Sigma^K_{i=1}p(C_i,x)$ で求める。クラス全体に関する和をとることから **周辺確率** という。 |
| $P(C_i)$ | **事前確率** 。 |

ベイズの定理を用いて識別を行う場合、**観測データ $x$ がクラス $C_i$ に分類される確率** 、 **観測データ $x$ が与えられた下でそれがクラス $C_i$ に属する条件付き確率** つまり **$P(C_i|x)$** を求めることになる。  
また、ベイズの定例は以下のように見ることもできる。

$$事後確率 = 修正項 \times 事前確率$$

### 3.1.2 ベイズの識別規則の例

以下の健康データがあったする。

||アンケート対象|喫煙する(S=1)|飲酒する(T=1)|
|:---:|:---:|:---:|:---:|
|健康(G=1)|800人|320人|640人|
|健康でない(G=0)|200人|160人|40人|
|計|1000人|480人|680人|

「喫煙も飲酒もするけど健康な人」の確率 $P(G=1|S=1,T=1)$ が知りたい。  
しかし、「健康な人で喫煙も飲酒もする人」の確率 $p(S=1,T=1|G=1)$ などは上記の表の情報から直接求めることができるが、「喫煙も飲酒もするけど健康な人」の確率 $P(G=1|S=1,T=1)$ は求めることができない。  
そこで、ベイズの定理を用いて以下のように導く。
ここでは分類するクラス $C$ を「健康」or「健康でない」、観測データ $x$ を「喫煙する・しない」「飲酒する・しない」と考える。

$$P(G=1|S=1,T=1) = \frac{p(S=1,T=1|G=1)}{p(S=1,T=1)} \times P(G=1)$$

上記についてそれぞれ求める。

$$p(S=1,T=1|G=1) = \frac{320}{800} \times \frac{640}{800} = \frac{8}{25}$$

$$P(G=1) = \frac{800}{1000} = \frac{4}{5}$$

「喫煙も飲酒もする人」の確率は、「健康な人で喫煙も飲酒もする人」の確率と「健康でない人で煙も飲酒もする人」の確率の和となる。つまり以下の式で表される。

$$p(S=1,T=1) = P(G=1) \times p(S=1,T=1|G=1)\\ + P(G=0) \times p(S=1,T=1|G=0)\\ = \frac{4}{5} \times \frac{8}{25}\\ + \frac{200}{1000} \times (\frac{160}{200} \times \frac{40}{200})\\ = \frac{32}{125} + \frac{4}{125}\\ = \frac{36}{125}$$

以上のことから「喫煙も飲酒もするけど健康な人」の確率 $P(G=1|S=1,T=1)$ は以下のようになる。

$$P(G=1|S=1,T=1) = \frac{8}{25} \times \frac{125}{36} \times \frac{4}{5}\\ = \frac{8}{9}$$

つまり、「喫煙も飲酒もする人」は $\frac{8}{9}$ の確率で「健康」に分類される。

### 3.1.3 尤度比

クラス $C_i$ とクラス $C_j$ の境界となる観測データ $x$ は以下の式から求められる。

$$P(C_i|x) = P(C_j|x)$$

これにベイズの定理を適用すると以下のようになる。

$$\frac{p(x|C_i)}{p(x)} \times P(C_i) = \frac{p(x|C_j)}{p(x)} \times P(C_j)$$

$$p(x|C_i) \times P(C_i) = p(x|C_j) \times P(C_j)$$

観測データ $x$ が $C_i$ に分類される場合は $p(x|C_i) \times P(C_i) > p(x|C_j) \times P(C_j)$  
観測データ $x$ が $C_j$ に分類される場合は $p(x|C_i) \times P(C_i) < p(x|C_j) \times P(C_j)$  
となる。  
**尤度比** を $h_{ij} = \frac{p(x|C_i)}{p(x|C_j)}$ とすると以下のようになる。

$$h_{ij} = \frac{p(x|C_i)}{p(x|C_j)} = \frac{P(C_j)}{P(C_i)}$$

つまり、 $\frac{P(C_j)}{P(C_i)}$ は定数値で求められ、これが閾値となる。  
尤度比 $h_{ij}$ が閾値を超えれば $C_i$ に分類され、閾値を下回れば $C_j$ に分類される。

### 3.1.4 ベイズの識別規則は誤り率最小

3章、本節以降やってない。

# 第 4 章：確率モデルと識別関数

## 4.1 節

やってない

## 4.2 確率モデル

- パラメトリックモデル
  - 学習データから推定した統計量（パラメータ）を用いて構成した確率モデルで分布を表現する
  - 確率変数が離散値の場合：二項分布、多項分布、ポアソン分布、など
  - 確率変数が連続値の場合：一様分布、指数分布、正規分布、など
  - 尤度推定法：確率モデルのパラメータを求める手法。尤度を最大にするパタメータとして算出する。
- ノンパラメトリックモデル
  - 特定の確率モデルを仮定せず、学習データそのものを用いてデータの分布を表現する
  - ヒストグラム法、k最近傍法（kNN法）、パルツェン（Przen）密度推定法、など

# 第 5 章：k最近傍法（kNN法）

k-meansとは違う。ちゃんと見てない。

# 第 6 章：線形識別関数

- 2乗誤差最小化基準
- フィッシャーの判別関数
- ロジスティック回帰

# 第 7 章：パーセプトロン型学習規則

# 第 8 章：サポートベクトルマシン

2クラス分類問題。

- カーネルトリック
- ν-サポートベクトルマシン（にゅー）
- 1クラスサポートベクトルマシン

# 第 9 章：部分空間法

次元削減。

- 主成分分析
- カーネル主成分分析
- カーネル部分空間法

# 第 10 章：クラスタリング

- 非階層的クラスタリング
  - k-means
- 階層的クラスタリング
  - 融合法
- 確率モデルによるクラスタリング
  - EMアルゴリズム

# 第 11 章：識別器の組み合わせによる性能強化

- 決定木
- バギング（bagging）
  - 複数の識別器を組み合わせる方法
- アダブースト（AdaBoost）
  - ブースティングアルゴリズム
  - ブースティング（boosting）は、複数の弱識別器を用意して学習を直列的にし、前の弱識別器の学習結果を参考にしながら一つずつ弱識別器を学習する
- ランダムフォレスト
  - バギングを強化した手法

# 付録：ベクトルと行列による微分
