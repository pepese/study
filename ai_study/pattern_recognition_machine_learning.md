パターン認識と機械学習  
略称「 **PRML** 」。

- 上：1〜5章
- 下：6章

# 第1章　序論
## 1.1　例：多項式曲線フィッティング
## 1.2　確率論
### 1.2.1　確率密度
### 1.2.2　期待値と分散
### 1.2.3　ベイズ確率
### 1.2.4　ガウス分布
### 1.2.5　曲線フィッティング再訪
### 1.2.6　ベイズ曲線フィッティング
## 1.3　モデル選択
## 1.4　次元の呪い
## 1.5　決定理論
### 1.5.1　誤識別率の最小化
### 1.5.2　期待損失の最小化
### 1.5.3　棄却オプション
### 1.5.4　推論と決定
### 1.5.5　回帰のための損失関数
## 1.6　情報理論
### 1.6.1　相対エントロピーと相互情報量
# 第2章　確率分布
## 2.1　二値変数
### 2.1.1　ベータ分布
## 2.2　多値変数
### 2.2.1　ディリクレ分布
## 2.3　ガウス分布
### 2.3.1　条件付きガウス分布
### 2.3.2　周辺ガウス分布
### 2.3.3　ガウス変数に対するベイズの定理
### 2.3.4　ガウス分布の最尤推定
### 2.3.5　逐次推定
### 2.3.6　ガウス分布に対するベイズ推論
### 2.3.7　スチューデントのt分布
### 2.3.8　周期変数
### 2.3.9　混合ガウス分布
## 2.4　指数型分布族
### 2.4.1　最尤推定と十分統計量
### 2.4.2　共役事前分布
### 2.4.3　無情報事前分布
## 2.5　ノンパラメトリック法
### 2.5.1　カーネル密度推定法
### 2.5.2　最近傍法
# 第3章　線形回帰モデル
## 3.1　線形基底関数モデル
### 3.1.1　最尤推定と最小二乗法
### 3.1.2　最小二乗法の幾何学
### 3.1.3　逐次学習
### 3.1.4　正則化最小二乗法
### 3.1.5　出力変数が多次元の場合
## 3.2　バイアス--バリアンス分解
## 3.3　ベイズ線形回帰
### 3.3.1　パラメータの分布
### 3.3.2　予測分布
### 3.3.3　等価カーネル
## 3.4　ベイズモデル比較
## 3.5　エビデンス近似
### 3.5.1　エビデンス関数の評価
### 3.5.2　エビデンス関数の最大化
### 3.5.3　有効パラメータ数
## 3.6　固定された基底関数の限界
# 第4章　線形識別モデル
## 4.1　識別関数（判別関数）
### 4.1.1　2クラス
### 4.1.2　多クラス
### 4.1.3　分類における最小二乗
### 4.1.4　フィッシャーの線形判別
### 4.1.5　最小二乗との関連
### 4.1.6　多クラスにおけるフィッシャーの判別
### 4.1.7　パーセプトロンアルゴリズム
## 4.2　確率的生成モデル
### 4.2.1　連続値入力
### 4.2.2　最尤解
### 4.2.3　離散特徴
### 4.2.4　指数型分布族
## 4.3　確率的識別モデル
### 4.3.1　固定基底関数
### 4.3.2　ロジスティック回帰
### 4.3.3　反復再重み付け最小二乗
### 4.3.4　多クラスロジスティック回帰
### 4.3.5　プロビット回帰
### 4.3.6　正準連結関数
## 4.4　ラプラス近似
### 4.4.1　モデルの比較とBIC
## 4.5　ベイズロジスティック回帰
### 4.5.1　ラプラス近似
### 4.5.2　予測分布
# 第5章　ニューラルネットワーク
## 5.1　フィードフォワードネットワーク関数
### 5.1.1　重み空間対称性
## 5.2　ネットワーク訓練
### 5.2.1　パラメータ最適化
### 5.2.2　局所二次近似
### 5.2.3　勾配情報の利用
### 5.2.4　勾配降下最適化
## 5.3　誤差逆伝播
### 5.3.1　誤差関数微分の評価
### 5.3.2　単純な例
### 5.3.3　逆伝播の効率
### 5.3.4　ヤコビ行列
## 5.4　ヘッセ行列
### 5.4.1　対角近似
### 5.4.2　外積による近似
### 5.4.3　ヘッセ行列の逆行列
### 5.4.4　有限幅の差分による近似
### 5.4.5　ヘッセ行列の厳密な評価
### 5.4.6　ヘッセ行列の積の高速な計算
## 5.5　ニューラルネットワークの正則化
### 5.5.1　無矛盾なガウス事前分布
### 5.5.2　早期終了
### 5.5.3　不変性
### 5.5.4　接線伝播法
### 5.5.5　変換されたデータを用いた訓練
### 5.5.6　たたみ込みニューラルネットワーク
### 5.5.7　ソフト重み共有
## 5.6　混合密度ネットワーク
## 5.7　ベイズニューラルネットワーク
### 5.7.1　パラメータの事後分布
### 5.7.2　超パラメータ最適化
### 5.7.3　クラス分類のためのベイズニューラルネットワーク
# 第6章　カーネル法
## 6.1　双対表現
## 6.2　カーネル関数の構成
## 6.3　RBFネットワーク
### 6.3.1　Nadaraya--Watsonモデル
## 6.4　ガウス過程
### 6.4.1　線形回帰再訪
### 6.4.2　ガウス過程による回帰
### 6.4.3　超パラメータの学習
### 6.4.4　関連度自動決定
### 6.4.5　ガウス過程による分類
### 6.4.6　ラプラス近似
### 6.4.7　ニューラルネットワークとの関係
# 第7章　疎な解を持つカーネルマシン
## 7.1　最大マージン分類器
### 7.1.1　重なりのあるクラス分布
### 7.1.2　ロジスティック回帰との関係
### 7.1.3　多クラスSVM
### 7.1.4　回帰のためのSVM
### 7.1.5　計算論的学習理論
## 7.2　関連ベクトルマシン
### 7.2.1　回帰問題に対するRVM
### 7.2.2　疎性の解析
### 7.2.3　分類問題に対するRVM
# 第8章　グラフィカルモデル
## 8.1　ベイジアンネットワーク
### 8.1.1　例：多項式曲線フィッティング
### 8.1.2　生成モデル
### 8.1.3　離散変数
### 8.1.4　線形ガウスモデル
## 8.2　条件付き独立性
### 8.2.1　3つのグラフの例
### 8.2.2有向分離（D分離）
## 8.3　マルコフ確率場
### 8.3.1　条件付き独立性
### 8.3.2　分解特性
### 8.3.3　例：画像のノイズ除去
### 8.3.4　有向グラフとの関係
## 8.4　グラフィカルモデルにおける推論
### 8.4.1連鎖における推論
### 8.4.2　木
### 8.4.3　因子グラフ
### 8.4.4　積和アルゴリズム
### 8.4.5　max-sumアルゴリズム
### 8.4.6　一般のグラフにおける厳密推論
### 8.4.7　ループあり確率伝播
### 8.4.8　グラフ構造の学習
# 第9章　混合モデルとEM
## 9.1　K-meansクラスタリング
### 9.1.1　画像分割と画像圧縮
## 9.2　混合ガウス分布(Mixtures　of　Gaussians)
### 9.2.1　最尤推定
### 9.2.2　混合ガウス分布のEMアルゴリズム
## 9.3　EMアルゴリズムのもう一つの解釈
### 9.3.1　混合ガウス分布再訪
### 9.3.2　K-meansとの関連
### 9.3.3　混合ベルヌーイ分布
### 9.3.4　ベイズ線形回帰に関するEMアルゴリズム
## 9.4　一般のEMアルゴリズム
# 第10章　近似推論法
## 10.1　変分推論
### 10.1.1　分布の分解
### 10.1.2　分解による近似のもつ性質
### 10.1.3　例：一変数ガウス分布
### 10.1.4　モデル比較
## 10.2　例：変分混合ガウス分布
### 10.2.1　変分事後分布
### 10.2.2　変分下界
### 10.2.3　予測分布
### 10.2.4　混合要素数の決定
### 10.2.5　導出された分解
## 10.3　変分線形回帰
### 10.3.1　変分分布
### 10.3.2　予測分布
### 10.3.3　変分下界
## 10.4　指数型分布族
### 10.4.1　変分メッセージパッシング
## 10.5　局所的変分推論法
## 10.6　変分ロジスティック回帰
### 10.6.1　変分事後分布
### 10.6.2　変分パラメータの最適化
### 10.6.3　超パラメータの推論
## 10.7　EP法
### 10.7.1　例：雑音データ問題
### 10.7.2　グラフィカルモデルとEP法
# 第11章　サンプリング法
## 11.1　基本的なサンプリングアルゴリズム
### 11.1.1　標準的な分布
### 11.1.2　棄却サンプリング
### 11.1.3　適応的棄却サンプリング
### 11.1.4　重点サンプリング
### 11.1.5　SIR
### 11.1.6　サンプリングとEMアルゴリズム
## 11.2　マルコフ連鎖モンテカルロ
### 11.2.1　マルコフ連鎖
### 11.2.2　Metropolis--Hastingsアルゴリズム
## 11.3　ギブスサンプリング
## 11.4　スライスサンプリング
## 11.5　ハイブリッドモンテカルロアルゴリズム
### 11.5.1　力学系
### 11.5.2　ハイブリッドモンテカルロアルゴリズム
## 11.6　分配関数の推定
# 第12章　連続潜在変数
## 12.1　主成分分析
### 12.1.1　分散最大化による定式化
### 12.1.2　誤差最小化による定式化
### 12.1.3　主成分分析の応用
### 12.1.4　高次元データに対する主成分分析
## 12.2　確率的主成分分析
### 12.2.1　最尤法による主成分分析
### 12.2.2　EMアルゴリズムによる主成分分析
### 12.2.3　ベイズ的主成分分析
### 12.2.4　因子分析
## 12.3　カーネル主成分分析
## 12.4　非線形潜在変数モデル
### 12.4.1　独立成分分析
### 12.4.2　自己連想ニューラルネットワーク
### 12.4.3　非線形多様体のモデル化
# 第13章　系列データ
## 13.1　マルコフモデル
## 13.2　隠れマルコフモデル
### 13.2.1　HMMの最尤推定
### 13.2.2　フォワード--バックワードアルゴリズム
### 13.2.3　HMMの積和アルゴリズム
### 13.2.4　スケーリング係数
### 13.2.5　Viterbiアルゴリズム
### 13.2.6　隠れマルコフモデルの拡張
## 13.3　線形動的システム
### 13.3.1　LDSにおける推論
### 13.3.2　LDSの学習
### 13.3.3　LDSの拡張
### 13.3.4　粒子フィルタ
# 第14章　モデルの結合
## 14.1　ベイズモデル平均化
## 14.2　コミッティ
## 14.3　ブースティング
### 14.3.1　指数誤差の最小化
### 14.3.2　ブースティングのための誤差関数
## 14.4　木構造モデル
## 14.5　条件付き混合モデル
### 14.5.1　線形回帰モデルの混合
### 14.5.2　ロジスティックモデルの混合
### 14.5.3　混合エキスパートモデル
