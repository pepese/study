自然言語処理 （NLP : Natural Language Processing）
===

要素技術の全体像は[ここ](https://www.slideshare.net/pfi/ss-11474303#7)。  
一部表に整理したのが以下。

<table>
<tr><td rowspan="2">単語分析</td><td>形態素解析   </td><td>大量の辞書と文法知識を使って品詞に分解   </td><td>辞書・文法アプローチ</td></tr>
<tr>                            <td>N-gram解析   </td><td>機械的に文をN個に分類                  </td><td>数学的アプローチ  </td></tr>
<tr><td>構文解析            </td><td>形態素解析   </td><td>品詞情報から係り受け判別を行い文構造を解析</td><td>辞書・文法アプローチ</td></tr>
<tr><td rowspan="2">意味分析</td><td>シソーラス解析 </td><td>大量の類義語・反義語辞書（シソーラス）を使って出現語をグルーピング化</td><td>辞書・文法アプローチ</td></tr>
<tr>                            <td>ベクトル空間解析</td><td>統計解析的手法を使って語の近傍度を数学的に計算</td><td>数学的アプローチ  </td></tr>
</table>

# 基礎技術

- 単語分割
  - 形態素解析
    - 入力文を言語学的に意味を持つ最小単位である形態素（一般的単語よりやや細かい単位）に分割
    - 各形態素の品詞を決定
    - 活用などの語形変化をしている形態素に対しては原型を割り当て
  - N-gram解析 （N文字インデックス法、Nグラム法）
    - 「ある文字列の中で、n個の文字列または単語の組み合わせが、どの程度出現するか」を調査する言語モデル
    - 検索対象を単語単位ではなく文字単位で分解し、後続の N-1 文字を含めた状態で出現頻度を求める
    - N-gram解析のイメージは[ここ](https://www.slideshare.net/kazoo04/ss-56821735#31)
    - Nの値が1なら「ユニグラム( **uni-gram** )」、2なら「バイグラム( **bi-gram** )」、3なら「トライグラム( **tri-gram** )」と呼ばれる
    - 形態素解析と異なり、辞書が不要、検索漏れが少ないなどのメリットがあるが、検索ノイズが多く検索速度が遅いなどのデメリットもある
- 構文解析
  - 文のタイプ（疑問文や命令文）を判別
  - 動詞句や名詞句といった句ごとのまとまりを見つける
  - 語と語の係り受け関係を調べる
- 重要語句の抽出
  - どのような内容が多いか少ないか、増えているか減っているか
  - どのような内容とどのような内容が統計的に関連性が高いか
  - 文章の内容として、どのような語句をどのような単位で抽出するかが分析結果の有効性に大きく影響
  - 抽出対象はキーワード
    - 出現頻度の高い語
    - 同じ頻度であれば、動詞より名詞
    - 係り受け関係をもつ体言と用言の組み合わせ
  - 問題点
    - 頻度が低いと、重要性が高くても、分析結果から外れてしまう
    - 頻度が高くても、重要性が低い語であれば分析結果における有用性が低くなる
    - そこで、抽出の単位をどうとるかで頻度の調整
      - 頻度が低すぎる　→　より短い単位に
      - 頻度が高すぎる　→　より長い単位に
      - 例:　「東京」　「基礎」　「研究所」 →　「基礎研究所」や「東京基礎研究所」に
- 語句内容の意味的役割の認識
  - 述語の分類　→　付属語情報を利用
  - 名詞概念　→　固有名詞や単位を伴う数値情報の抽出
  - 特定の目的に添った語句　→　テンプレートパターンとのマッチングによる５W1Hとして抽出
- 同義性の認識
  - テキストマイニングでは、表現の多様性を吸収するための技術が重要
  - 同義語辞書のような知識を参照
  - 各語がどのような語と係受けを結ぶかを構文解析により対象データから抽出
- データマイニング
  - 相関ルール（association rule）
    - 全文データベースシステムに蓄積された全テキスト集合の中から、XとYをともに含むテキストに成り立つ関係を扱う
  - 支持度（X⇒Y）
    - データベース全体の中でXとYをともに含むテキスト集合の割合
  - 確信度
    - データベース全体の中でXを含むテキスト集合のうち、XとYをともに含むテキスト集合の割合
  - 相関ルールX⇒Y
    - 単語集合Xと単語集合Yが、テキスト集合間で共起関係（cooccurrence)を示す
  - 検索式記述の修正に用いる
    - 全文データベースに格納された文書間の相関関係を示す

参考
- [心理データ解析演習（心理デザインデータ解析演習）](http://cogpsy.educ.kyoto-u.ac.jp/personal/Kusumi/datasem.htm)
  - [心理データ解析演習：第５回
テキストマイニング入門](http://cogpsy.educ.kyoto-u.ac.jp/personal/Kusumi/datasem13/oka.pdf)
- [心理学のためのデータ解析法](http://cogpsy.educ.kyoto-u.ac.jp/personal/Kusumi/kaiseki.htm)


## 形態素解析

代表的なツールに **MeCab** 、 **JUMAN** 、茶筌( **ChaSen** )、 **KAKASI** がある。

<table>
<tr class="even">
<td align="center"></td>
<td align="center"><b>MeCab</b></td>
<td align="center"><a href=
"http://chasen.naist.jp/">ChaSen</a></td>
<td align="center"><a href="http://pine.kuee.kyoto-u.ac.jp/nl-resource/juman.html">JUMAN</a></td>
<td align="center"><a href="http://kakasi.namazu.org">KAKASI</a></td>
</tr>
<tr class="odd">
<td align="center">解析モデル</td>
<td align="center">bi-gram マルコフモデル</td>
<td align="center">可変長マルコフモデル</td>
<td align="center">bi-gram マルコフモデル</td>
<td align="center">最長一致</td>
</tr>
<tr class="even">
<td align="center">コスト推定</td>
<td align="center">コーパスから学習</td>
<td align="center">コーパスから学習</td>
<td align="center">人手</td>
<td align="center">コストという概念無し</td>
</tr>
<tr class="odd">
<td align="center">学習モデル</td>
<td align="center"><a href="http://www.cis.upenn.edu/~pereira/papers/crf.pdf">CRF</a> (識別モデル)</td>
<td align="center">HMM (生成モデル)</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">辞書引きアルゴリズム</td>
<td align="center">Double Array</td>
<td align="center">Double Array</td>
<td align="center">パトリシア木</td>
<td align="center">Hash?</td>
</tr>
<tr class="odd">
<td align="center">解探索アルゴリズム</td>
<td align="center">Viterbi</td>
<td align="center">Viterbi</td>
<td align="center">Viterbi</td>
<td align="center">決定的?</td>
</tr>
<tr class="even">
<td align="center">連接表の実装</td>
<td align="center">2次元 Table</td>
<td align="center">オートマトン</td>
<td align="center">2次元 Table?</td>
<td align="center">連接表無し?</td>
</tr>
<tr class="odd">
<td align="center">品詞の階層</td>
<td align="center">無制限多階層品詞</td>
<td align="center">無制限多階層品詞</td>
<td align="center">2段階固定</td>
<td align="center">品詞という概念無し?</td>
</tr>
<tr class="even">
<td align="center">未知語処理</td>
<td align="center">字種 (動作定義を変更可能)</td>
<td align="center">字種 (変更不可能)</td>
<td align="center">字種 (変更不可能)</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">制約つき解析</td>
<td align="center">可能</td>
<td align="center">2.4.0で可能</td>
<td align="center">不可能</td>
<td align="center">不可能</td>
</tr>
<tr class="even">
<td align="center">N-best解</td>
<td align="center">可能</td>
<td align="center">不可能</td>
<td align="center">不可能</td>
<td align="center">不可能</td>
</tr>
</table>

### [Mecab](http://taku910.github.io/mecab/)

オープンソースの形態素解析エンジン。  
**条件付き確率場** と呼ばれる数理モデルを利用して、単語と単語間の繋がりの自然さを計算し、一番コスト（単語コスト、連結コスト）の低い、単語の列を選択するようにしている。  
単語ごとにコストを定義しておく必要があり、単語生成のコストが低いほど、その単語は選ばれやすくなる。

#### 辞書の種類

Mecabには、有志の方が固有名詞に対応したipadic-neologd辞書などいくつか種類がある。

- IPA辞書（IPADIC）
- [mecab-ipadic-NEologd](https://github.com/neologd/mecab-ipadic-neologd)
  - 有志によりIPA辞書が強化された辞書。新語・固有表現に強い。
- NAIST
  - IPAdic の後継。IPA辞書の固有名詞以外の全エントリをチェック（可能性に基づく品詞の整理）し、表記ゆれ情報を付与し、複合語の構造を付与する作業が行われている。
  - 固有名詞については不要な語、新規追加などの整理が随時行われている。
  - この作業により IPA辞書 のライセンスで問題となっていた ICOT 条項を削除し、広告条項無しの BSD ライセンスに変更された。
- UniDic
  - IPA 辞書よりも個々の単語を詳細に分類したもので、分割した形態素が文中で果たす役割をより精密に検出することができる。
  - UniDic現代語版
  - UniDic近代文語版
- JUMAN
- Yahoo形態素解析
- [IPA、NAIST、UniDic、JUMANの辞書実演比較](http://www.mwsoft.jp/programming/munou/mecab_dic_perform.html)

#### 辞書への単語の追加

- https://taku910.github.io/mecab/dic.html


## 構文解析

文章を構文木（Syntactic Tree）にして、その文章がどのような文構造を持つかを明らかにする。
文法規則によって、文の構造を句・文節を単位として解析する。  
句とは、2つ以上の語が集まって1つの品詞と同じような働きをしながら、文を構成する語の塊のことである。  
名詞の役割を果たす句を名詞句(NP)、動詞の役割を果たす句を動詞句(VP)とするように、形容詞句(ADJP)、副詞句(ADVP)などがある。  
英語では、句構造で構文解析を行うが、日本語の場合は、文節を単位に係り受け関係を用いて構文を解析するのが一般的である。  
係り受け関係を解析するフリーソフトとしては、JUMANをベースとした「 **KNP** 」、茶筌をベースとした「南瓜( **CaboCha** )」がある。

- KNP (日本語構文・格・照応解析システム)
  - 日本語文の構文・格・照応解析を行うシステム
  - JUMANの解析結果(形態素列)を入力とし, 文節および基本句間の係り受け関係，格関係，照応関係を出力
  - 係り受け関係，格関係および照応関係は，Webから自動構築した大規模格フレームに基づく確率的モデルにより決定
- CaboCha/南瓜
  - Support Vector Machines に基づく日本語係り受け解析器

## 意味解析・意味理解

文中の単語は何らかの意味をもち、また文中で依存関係にある２語の間には何らかの意味関係がある。  
しかし、それらの意味はその単語だけ、あるいは依存関係を示す表現だけを見ていたのでは必ずしも一意に決まらない。  
これは、単語には複数の意味をもつものがあり、依存関係を示す表現、例えば名詞をつなぐ「AのB」という表現が示す意味関係にも複数ありえるからである。  
このような曖昧性の中には、文脈情報がなければ解釈できないもの、あるいは文脈によってもわからないものがある。

- 語義曖昧性解消 WSD (Word Sense Disambiguation)
- 述語項構造解析
- テキスト含意認識
- 格文法
- フレーム意味論
- 述語項構造解析ツール
  - 文章中に出現する述語とその格要素を同定するツール
    - SynCha 日本語の述語項構造解析器
    - ChaPAS Javaベースの日本語述語項構造解析器
    - YuCha 日本語述語項構造解析器「夕茶」
- 意味解析ツール
  - シソーラスを用いて、文章中に出現する述語とその格要素を分類するツール。フレーム意味論といった高次までは難しい。
    - 意味解析システムSAGE
    - 日本語 意味解析ツール AYA
- ベクトル空間解析による意味解析
  - 潜在意味解析（LSA: Latent Semantic Analysis）
  - ベクトル空間モデル
  - TF-IDF（tf\*idf法）
    - 単語の重み付けにより文を抽出する手法
    - *tf* と *idf* の積の値により文を抽出する
      - tf（term frequency）： ある文書における ある単語の出現頻度
      - idf（inverse document frequency） ： 文書頻度の逆数
        - 「文書の数」を「ある単語を含む文書数」で割ったものに対数をとる
  - word2vec
    - http://business.nikkeibp.co.jp/article/bigdata/20141110/273649/

# テキストマイニング

テキストマイニングはデータマイニングの派生語であり、1990年代の後半に用いられた新しい用語で、膨大に蓄積されたテキストデータを何らかの単位(文字、単語、フレーズ)に分解し、これらの関係を定量的に分析することである。  
しかし、テキストを構成する要素を定量的に分析する手法は、テキストマイニングという用語が使われる、はるか前から計量文体分析のような分野で用いられてきた。

- 計算的テキスト解析(computational text analysis)
- 統計的テキスト解析(statistical text analysis)
  - 計量文体学
    - 文章を構成する要素(文字、単語、文節、文、段落など)について統計的に集計分析を行い、文章のジャンルの特徴や個別作家の文体の特徴を定量的に解析する
  - 計量言語学とコーパス言語学
    - 言語現象を定量的に分析する分野
    - コーパス(corpus)とは、言語を分析のために集めた言語資料のこと


## **1-of-N**

テキストマイニングにおいて、単語や文章を解析するためには何らかの方法で数値データに変換する必要がある。　　
例えば、辞書に単語が「私」「あなた」「蹴る」の3つしかなく（N=3）この順だとすると、それぞれの単語は以下のベクトルで表現できる。

- 「私」 = [1, 0, 0]^T
- 「あなた」 = [0, 1, 0]^T
- 「蹴る」 = [0, 0, 1]^T

上記の通り、その単語に対応するベクトルの要素を「1」とし、それ以外は「0」とする。  
Nのサイズつまりベクトルの次元は、「辞書に含まれる単語の数」や「解析対象の文章に含まれる単語の数」になる。  
このように単語をベクトルに変換する方法を **1-of-N** 表現という。

## **bag-of-words**

1-of-N の拡張として文章をベクトル表現する方法に **bag-of-words** がある。  
これは、文章を構成する単語の 1-of-N 表現のベクトルを加算する方法である。  
例えば、「私はあなたを蹴る」は以下のようになる。

- 「私はあなたを蹴る」
- 「私」「あなた」「蹴る」
- [1, 0, 0]^T + [0, 1, 0]^T + [0, 0, 1]^T = [1, 1, 1]^T

## テキストマイニングの分析の種類

- クラスター分析（[参考１](http://www.kamishima.net/jp/clustering/)、[参考２](http://www.kamishima.net/archive/clustering.pdf)）
  - 階層的クラスター分析
    - 分割型 (divisive)
      - データ集合全体が一つのクラスタの状態から，順次クラスタを分割して，クラスタの階層を生成する。
    - 凝集型 (agglomerative)
      - N個の対象からなるデータが与えられたとき，1個の対象だけを含む N個のクラスタがある初期状態をまず作り、この状態から始めて，クラスタ間の距離(非類似度)を計算し，最も距離の近い二つのクラスタを逐次的に併合していく。
      - 目的のクラスタ数になるまでグルーピングを繰り返し、最終的に1つのクラスタになる。
      -　デンドログラムと呼ばれる樹形図で表現され、結びつきの階層構造を明確にできる。
  - 非階層的クラスター分析
    - 分割の数などを与えて全体を一気に分割する方法。
    - 最適な分割数を決めるための仮定などを予め決める必要がある。
    - 結びつきの階層構造は分からない。
    - k-means法などが該当する。
- ネットワーク分析
- 主成分分析
- 対応分析
- 潜在的意味解析
  - トピックモデル（！！！[トピックモデルの話](https://www.slideshare.net/kogecoo/ss-47464673)！！！）
    - 教師無しクラスタリング
    - データの背後にある隠れた「トピック」を推定する
    - 自然言語処理で潜在意味解析という文脈で発展
    - 「トピック」や「潜在的意味」は「話題や分野」と考えるとよい
      - LSI（Latent Semantic Indexing）
        - 文書-語彙の共起行列ベース
      - PLSI（Probabilistic LSI）
      - LDA（Latent Dirichlet Allocation）
- etc…

### 階層的クラスター分析

手順

1. すべてのクラスターの組(初めは要素)に対して、クラスター間の距離(非類似性)を求める
2. クラスター間の距離(非類似性)を参照してクラスター間距離が最小のクラスターの組を結合し、新たなクラスターを作成する
3. 新たなクラスターとその他のクラスター間の距離(更新距離)を求める
4. クラスター数があらかじめ決められた数(通常は１)になるまで、2・3を繰り返す

文字データをどのように数値データとしてコードすればよいのか？  
先に紹介した **bag-of-words** を利用することで、テキストマイニングにおいてある文章(s_n)は、全文中に含まれるすべての単語を要素(n_n, v_n)とするベクトルとして表現できる。

|文章|高木（n_1）|田中（n_2）|尾崎（n_3）|ボール（n_4）|バット（n_5）|蹴る（v_1）|打つ（v_2）|
|---|---|---|---|---|---|---|---|
|高木がボールを蹴った（s_1）      |1|0|0|1|0|1|0|
|田中がバールを蹴った（s_2）      |0|1|0|1|0|1|0|
|尾崎がバットでボールを打った（s_3）|0|0|1|1|1|0|1|
|高木が田中を蹴った（s_4）        |1|1|0|0|0|1|0|
|田中が高木を蹴った（s_5）        |1|1|0|0|0|1|0|

クラスター分析では要素間の「非類似性」をもとにクラスタリングを行うことが分かっている。

- ２値データの場合の類似度： **Jaccard係数** （ **ジャッカール** ）
  - 集計したデータが２値データの場合や、間隔尺度のデータである場合は、それにあった非類似性の指標を用いる必要がある。
  - Jaccard係数は上記のようなデータを扱う際の「類似性」の指標。
- ２値データの場合の非類似度： **Jaccard距離**
  - Jaccard距離＝1 - Jaccard係数

ある要素xとyの類似度を算出する場合、「少なくともxもしくはyを含む集合の数（＝x ∪ y）」と「xとyの両方を含む集合の数（＝x ∩ y）」を用いて、類似度つまりJaccard係数は、「 **(x ∩ y)/(x ∪ y)** 」で算出できる。  
例えば「高木（n_1）」と「田中（n_2）」の類似度・非類似度を算出すると以下のようになる。

- n_1 ∩ n_2　＝　s_4　と　s_5 の2つ　＝　2
- n_1 ∪ n_2　＝　s_1　と　s_2 と s_4　と　s_5 の4つ　＝　4
- Jaccard係数　＝　(n_1 ∩ n_2)/(n_1 ∪ n_2)　＝　2/4　＝　0.50
- Jaccard距離　＝　1 - Jaccard係数　＝　1 - 0.50　＝　0.50

以上のことから、 n_1～n_5,v_1,v_2 についてJaccard係数の行列は以下のようになる。

|   |n_1 |n_2 |n_3 |n_4 |n_5 |v_1 |v_2 |
|---|----|----|----|----|----|----|----|
|n_1|-   |    |    |    |    |    |    |
|n_2|0.50|-   |    |    |    |    |    |
|n_3|0.00|0.00|-   |    |    |    |    |
|n_4|0.20|0.20|0.33|-   |    |    |    |
|n_5|0.00|0.00|1.00|0.33|-   |    |    |
|v_1|0.75|0.75|0.00|0.40|0.00|-   |    |
|v_2|0.00|0.00|1.00|0.33|1.00|0.00|-   |

さらにJaccard距離は以下の行列になる。

|   |n_1 |n_2 |n_3 |n_4 |n_5 |v_1 |v_2 |
|---|----|----|----|----|----|----|----|
|n_1|-   |    |    |    |    |    |    |
|n_2|0.50|-   |    |    |    |    |    |
|n_3|1.00|1.00|-   |    |    |    |    |
|n_4|0.80|0.80|0.67|-   |    |    |    |
|n_5|1.00|1.00|0.00|0.67|-   |    |    |
|v_1|0.25|0.25|1.00|0.60|1.00|-   |    |
|v_2|1.00|1.00|0.00|0.67|0.00|1.00|-   |

上記の内、「n_3 と n_5」、「n_2 と v_2」、「n_5 と v_2」のJaccard距離が 0.00 のため、「n_3 と n_5 と v_2」は1つのクラスタとなる。  
これを **c_1** とした場合、「ある文章に特定の単語が含まれているか(1)、いないか(0)」をデータすると以下のようになる。

|文章|高木（n_1）|田中（n_2）|尾崎・バット・打つ （c_1）|ボール（n_4）|蹴る（v_1）|
|----------------------------|---|---|---|---|---|---|---|
|高木がボールを蹴った（s_1）      |1|0|0|1|1|
|田中がバールを蹴った（s_2）      |0|1|0|1|1|
|尾崎がバットでボールを打った（s_3）|0|0|1|1|0|
|高木が田中を蹴った（s_4）        |1|1|0|0|1|
|田中が高木を蹴った（s_5）        |1|1|0|0|1|

上記から再度Jaccard距離を求めると以下のようになる。

|   |n_1 |n_2 |c_3 |n_4 |v_1 |
|---|----|----|----|----|----|
|n_1|-   |    |    |    |    |
|n_2|0.50|-   |    |    |    |
|c_1|1.00|1.00|-   |    |    |
|n_4|0.80|0.80|0.67|-   |    |
|v_1|0.25|0.25|1.00|0.60|-   |

上記から次は「0.25」となっているところがグルーピングできそうだが、どちらにしようか迷う。  
このようなときに *クラスター間の距離測定方法* を用いて判断を行う。

>コラム：文章のクラスター分析と特徴語のクラスター分析
> - 特徴語のクラスター分析(今回)
>   - 「ある単語をもとに、その単語が他の単語とどのように共起しているかをそれぞれの文章を参照して調べ、文章内での共起の頻度が高い順に単語のクラスターを形成する」
> - 文章のクラスター分析
>   - 「ある文章をもとに、その文章が含む単語がどのように共起しているかを調べ、他の似たような単語の共起を示す文章とクラスターを形成する」

>コラム：様々な距離
> - ユークリッド距離
>   - 所謂二乗の和の平方根
> - マハラノビス距離
> - コサイン係数
>   - コサイン類似度は、そのまま、ベクトル同士の成す角度の近さを表現するため、三角関数の普通のコサインの通り、1に近ければ類似しており、0に近ければ似ていないことになる。  
> [参考](http://www.albert2005.co.jp/technology/mining/cluster2.html)

クラスター間の距離測定方法には以下のようなものがある。

- Ward法（うぉーど）
  - 2つのクラスターP,Qを結合したと仮定したとき、「それにより移動したクラスターの重心とクラスター内の各サンプルとの距離の2乗和 **L(P∪Q)** 」と、「元々の2つのクラスター内での重心とそれぞれのサンプルとの距離の2乗和 **L(P)** 、 **L(Q)** 」の差「 **Δ= L(P∪Q)－L(P)－L(Q)** 」が最小となるようなクラスター同士を結合する手法。
  - Δの値を **情報ロス量** という。
  - 計算量は多いが分類感度がかなり良いため、よく用いられる。
- 群平均法
  - 各クラスター同士で、全ての組み合わせのサンプル間距離の平均をクラスター間距離とする手法。
  - 鎖効果や拡散現象を起こさないため、用いられることが多い。
- 最短距離法
  - 2つのクラスターのサンプル同士で最も小さいサンプル間距離をクラスター間の距離とする手法。
  - 鎖効果により、クラスターが帯状になってしまい、分類感度が低い。計算量が少ない。
- 最長距離法
  - 最短距離法の逆で各クラスター中、最大のサンプル間距離をクラスター間距離とする。
  - 分類感度は高いが、クラスター同士が離れてしまう拡散現象が生じる。計算量が少ない。

今回は **Ward法** でグルーピングを行う。  
「n_1 と v_1」、「n_2 と v_1」のそれぞれでΔを算出してみる。

- n_1 と v_1
  - L(n_1) = 0
    - n_1 = [s_1, s_2, s_3, s_4, s_5]^T = [1, 0, 0, 1, 1]^T
    - 上記の1要素しかないので重心との距離は0のため
  - L(v_1) = 0
    - v_1 = [s_1, s_2, s_3, s_4, s_5]^T = [1, 1, 0, 1, 1]^T
    - 上記の1要素しかないので重心との距離は0のため
  - L(n_1 ∪ v_1) =
    - 上記のn_1、v_1のベクトルを使用
    - 重心 = [1, 0.5, 0, 1, 1]^T
    - Δ = 0.25^2 + 0.25^2
- n_2 と v_1
  - L(n_2) = 0
    - n_2 = [s_1, s_2, s_3, s_4, s_5]^T = [0, 1, 0, 1, 1]^T
    - 上記の1要素しかないので重心との距離は0のため
  - L(v_1) = 0
    - v_1 = [s_1, s_2, s_3, s_4, s_5]^T = [1, 1, 0, 1, 1]^T
    - 上記の1要素しかないので重心との距離は0のため
  - L(n_2 ∪ v_1) =
    - 上記のn_2、v_1のベクトルを使用
    - 重心 = [0.5, 1, 0, 1, 1]^T
    - Δ = 0.25^2 + 0.25^2

今回はWard法で同じになってしまった。。。  
ここまでは、単語のクラスタリングを行ってきた。  
文章の場合は、文章（s_1～s_5）軸のベクトルを使用して、Jaccard距離を求めて、、、とやれば、文章のクラスタリングができる。

- [KH Coderを用いたテキストマイニング入門](http://cogpsy.educ.kyoto-u.ac.jp/personal/Kusumi/datasem13/oka.pdf)
- [KH Coder](http://khc.sourceforge.net/)

## ネットワーク分析

語の共起(共出現)パターン。
語の共起は、N-gramを含む広い意味での、語が文、あるいはテキストの中に同時用いられていることを指す。
N-gramモデルとは、「ある文字列の中で、n個の文字列または単語の組み合わせが、どの程度出現するか」を調査する言語モデルを意味する。
語のネットワークマップとは、基本的には、文、あるいはテキストの中で用いられた語をノードとし、同時に用いられた場合は、語と語を線(辺として)でリンクしたグラフである。


# その他の分野

## 文書要約

文書の要約には以下の2種類がある。

- 単一文書要約
  - 要約する文書が1つの場合
  - 長い文書の中から重要な部分だけを取り出す技術
  - 冗長な文書を簡潔に伝えることができる
  - **リード法** がよく使われる
    - 段落の先頭の文を重要視し，文を抽出する
- 複数文書要約
  - 要約する文書が複数の場合

さらに以下のような分類もある。

- 抽出型要約
  - 要約対象の文書から、文自体を変更せずに、文を抽出する
  - コンピュータで実用化されているのはほぼこれ
    - 内容を反映しながら文法的にも意味的にも自然な文を生成することが難しいため
  - 代表的な手法に **MMR** （maximal marginal relevance）がある
- 抽象型要約
  - 新しい文を生成しながら内容を抽象的に要約する
  - 実際に人間が行う方法



日本語要約の実装例には以下がある。

- [GitHub - recruit-tech/summpy](https://github.com/recruit-tech/summpy)


## 質問応答

- ファクトイド型質問
  - 「何」「誰」「いつ」「どこ」を含むような、特定の物や人、時間、場所などで答えられる質問
- ノンファクトイド型質問
  - 「なぜ」「どうやって」を含むような、理由や手順に関する質問


# 用語

- 分かち書き
  - 単に文が形態素で区切られた形
    - 「ＭｅＣａｂで形態素解析を行うとこうなる．」
    - => 「ＭｅＣａｂ　　で　　形態素　　解析　　を　　行う　　と　　こう　　なる」
- コーパス
  - 単なる文章の集合体ではなく、文章に対してその意味や構造などを付与した大規模なデータのこと。作成にはとてつもない人手と手間がかかる。
- シソーラス
  - シソーラス (Thesaurus) は単語の上位 / 下位関係、部分 / 全体関係、同義関係、類義関係などによって単語を分類し、体系づけた類語辞典・辞書。
  - 単語同士の関係を分類した辞書のこと。例えば、「リンゴ」という単語に対して、上位概念に「果物」「植物」、下位概念に「青りんご」や「ジョナゴールド」などが定義されている。

# その他メモ

- **★★SentencePiece★★**
  - http://qiita.com/halhorn/items/675be9559ed92e1c2049
  - http://b.hatena.ne.jp/entry/qiita.com/taku910/items/7e52f1e58d0ea6e7859c
- [自然言語処理における前処理の種類とその威力](http://qiita.com/Hironsan/items/2466fe0f344115aff177)
- [言語処理100本ノック 2015](http://www.cl.ecei.tohoku.ac.jp/nlp100/)
- [同志社大学 R、R言語、R環境・・・・・・](https://www1.doshisha.ac.jp/~mjin/R/)


分布一覧

|分布の総称|確率密度関数|乱数発生関数|
|:---|:---|:---|
|一様(Uniform)分布|dunif(x, min=0, max=1,･･･)|runif(n, min=0, max=1)|
|二項(Binomial)分布|dbinom(x, size, prob,･･･)|rbinom(n, size, prob)|
|ポアソン(Poisson)分布|dpois(x, lambda,･･･)|rpois(n, lambda)|
|正規(Normal)分布|dnorm(x, mean=0, sd=1,･･･)|rnorm(n, mean=0, sd=1)|
|カイ2乗(Chi-square )分布|dchisq(x, df, ncp=0,･･･)|rchisq(n, df, ncp=0)|
|t分布|dt(x, df,･･･)|rt(n, df)|
|F分布|df(x, df1, df2,･･･)|rf(n, df1, df2)|
|ガンマ(Gamma)分布|dgamma(x, shape,･･･)|rgamma(n, shape)|
|ベータ(Beta)分布|dbeta(x, shape1, shape2,･･･)|rbeta(n, shape1, shape2)|
|対数正規(Lognormal)分布|dlnorm(x, meanlog = 0, sdlog = 1,･･･)|rlnorm(n, meanlog = 0, sdlog = 1)|
|ロジスティック(Logistic)分布　|dlogis(x, ･･･)|rlogis(n)|
|指数(Exponential)分布|dexp(x, rate = 1, ･･･ )|rexp(n, rate = 1)|
|負二項(Negbinomail)分布|dnbinom(x, size, prob, mu,･･･ )|rnbinom(n, size, prob, mu)|
|多項(Multinomial)分布|dmultinom(x, prob, ･･･ )|rmultinom(n, size, prob)|
|幾何(Geometric)分布|dgeom(x, prob, ･･･ )|rgeom(n, prob)|
|超幾何(Hypergeometric)分布|dhyper(x, m, n, k, ･･･ )|rhyper(nn, m, n, k)|
|コーシー(Cauchy)分布|dcauchy(x,location=0,scale= 1,･･･ )|rcauchy(n,location=0,scale = 1)|
|ワイブル(Weibull)分布|dweibull(x,shape,scale=1,･･･ )|rweibull(n, shape, scale = 1)|
